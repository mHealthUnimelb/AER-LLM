{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module was compiled against NumPy C-API version 0x10 (NumPy 1.23) but the running NumPy has C-API version 0xf. Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module was compiled against NumPy C-API version 0x10 (NumPy 1.23) but the running NumPy has C-API version 0xf. Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem."
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import csv\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "import re \n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import ast \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "import eval_metrics as em\n",
    "from sklearn.metrics import recall_score, balanced_accuracy_score\n",
    "\n",
    "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
    "GOOGLE_API_KEY= 'AIzaSyBXOwIFMOC_CJ_1EfgbwYtpWuAvTOvNx90'\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"./goemotion_data/sample.csv\")\n",
    "example = pd.read_csv(\"./goemotion_data/example.csv\")\n",
    "example_dict = json.load(open(\"./goemotion_data/examples_dict.json\"))\n",
    "unique_labels = [\"neutral\", \"admiration\", \"gratitude\", \"approval\", \"amusement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_sample_probs = {}\n",
    "for i in range(210):\n",
    "    amb_labels = []\n",
    "    emotion_counts = Counter(sample.loc[i,\"human_label\"].split(\",\"))\n",
    "    total_counts = sum(emotion_counts.values())\n",
    "    probs = {emo: round(emotion_counts[emo] / total_counts, 4) for emo in unique_labels}\n",
    "    gt_sample_probs[sample.loc[i,\"id\"]] = [probs[emo] for emo in unique_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_label_prob(text_dict):\n",
    "#     for i, item in text_dict.items():\n",
    "#         emos = item['human_label']\n",
    "#         emo_counts = Counter(emos)\n",
    "#         total_count = sum(emo_counts.values())\n",
    "#         probs = {emo: round(emo_counts[emo]/total_count, 2) for emo in unique_labels}\n",
    "#         item['emo_dict'] = probs\n",
    "#     return text_dict\n",
    "\n",
    "# example.to_csv(\"./goemotion_data/example.csv\", index = False)\n",
    "\n",
    "# json.dump(example_dict, open(\"./goemotion_data/examples_dict.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_sampling(example, example_dict, num):\n",
    "    multi_num = num//3 + 1\n",
    "    single_num = num - multi_num\n",
    "\n",
    "    multi_df = example[example['type'] == 2]\n",
    "    single_df = example[example['type'] == 1]\n",
    "\n",
    "    multi_index = multi_df.sample(multi_num,  random_state=10, weights='weights')\n",
    "    single_index = single_df.sample(single_num,  random_state=10, weights = 'weights')\n",
    "\n",
    "    cur_example_df = pd.concat([multi_index, single_index]).reset_index()\n",
    "\n",
    "    example_text = \"Example: \\n\" + \"\\n\".join(f\"Example {i+1}: {cur_example_df.loc[i,'text']}. Emotion probabilities: {example_dict[cur_example_df.loc[i, 'id']]['emo_dict']}\" for i in list(cur_example_df.index))\n",
    "    return example_text, cur_example_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text, cur_example_df = example_sampling(example, example_dict, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example: \\nExample 1: Thanks for the advice I really appreciate it and I hope you\\'re doing better now!. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 2: Thanks man, I appreciate it, I love her but this was way to far, thanks man üíú. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 3: you\\'re hilarious. Thanks for the entertainment this morning.. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 4: This is amazing, man. Kick some ass, make good food, get paid. Congratulations. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 5: lmaooo you\\'re right I did mean [NAME] by that haha \"she\" lol. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 6: Ah, pillars of society. No wonder MLS is so highly regarded.. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 7: Please explain ‚ÄúHoover/ed‚Äù. Not familiar with the term. Thank you-. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 8: WHY IS THERE WOMAN ON MY ENTERTAINMENT??????????. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 9: Video games are great, but sometimes it‚Äôs nice to unplug and play a game of Jenga with your cat. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 10: What a baby.... Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 11: It\\'s a good thing that liopleurodon was actually only about six meters long and went extinct 155,000,000 years ago.... Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 12: Hahahahhahhaha!!! Thats the right wing in a nutshell...... Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 13: Correct. I still play NHL Faceoff 98 and I always turn that rule off, because it\\'s terrible.. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 14: Still can\\'t see. Thanks.. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 15: Thank you. Edit: I have cancer now. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_GENERATE_GOEMOTION = ' '.join([\n",
    "    \"You are an emotionally-intelligent and empathetic agent.\",\n",
    "    \"You will be given a piece of text and some examples, and your task is to predict the probability of the emotion of a target text. \"\n",
    "])\n",
    "RULES = \" \".join([\n",
    "    \"Choose the emotions from [neutral, admiration, gratitude, approval, amusement]. Output statisfies the following rules\\n\",\n",
    "    \"Rule 1: Generate a dictionary of emotion probabilities in format of {'neutral': 0.1, 'admiration':0.0, 'gratitude':0.1, 'approval':0.8, 'amusement':0.0}.\",\n",
    "    \"Rule 2: Ensure the sum of probability equal to 1.\\n\",\n",
    "    \"Rule 3: Do not explain, only the dictionary.\\n\",\n",
    "    \"Please check again whether your output follows the three rules.\"\n",
    "])\n",
    "USER_PROMPT_RETRY_INSTRUCTION = \"Please only pick from the given options separated by comma.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_pred_emotion(text, example_text):\n",
    "    time.sleep(0.2)\n",
    "    prompt = SYSTEM_PROMPT_GENERATE_GOEMOTION + example_text + f\"The target text is :{text}.\" + RULES\n",
    "    response = model.generate_content(prompt)\n",
    "    return response, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'neutral': 0.0, 'admiration': 0.2, 'gratitude': 0.0, 'approval': 0.5, 'amusement': 0.3} \\n\""
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response, prompt = gemini_pred_emotion(sample.loc[0, 'text'], example_text)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an emotionally-intelligent and empathetic agent. You will be given a piece of text and some examples, and your task is to predict the probability of the emotion of a target text. Example: \\nExample 1: Thanks for the advice I really appreciate it and I hope you\\'re doing better now!. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 2: Thanks man, I appreciate it, I love her but this was way to far, thanks man üíú. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 3: you\\'re hilarious. Thanks for the entertainment this morning.. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 4: This is amazing, man. Kick some ass, make good food, get paid. Congratulations. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 5: lmaooo you\\'re right I did mean [NAME] by that haha \"she\" lol. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 6: Ah, pillars of society. No wonder MLS is so highly regarded.. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 7: Please explain ‚ÄúHoover/ed‚Äù. Not familiar with the term. Thank you-. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 8: WHY IS THERE WOMAN ON MY ENTERTAINMENT??????????. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 9: Video games are great, but sometimes it‚Äôs nice to unplug and play a game of Jenga with your cat. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 10: What a baby.... Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 11: It\\'s a good thing that liopleurodon was actually only about six meters long and went extinct 155,000,000 years ago.... Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 12: Hahahahhahhaha!!! Thats the right wing in a nutshell...... Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 13: Correct. I still play NHL Faceoff 98 and I always turn that rule off, because it\\'s terrible.. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 14: Still can\\'t see. Thanks.. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}\\nExample 15: Thank you. Edit: I have cancer now. Emotion probabilities: {\\'neutral\\': 0.0, \\'admiration\\': 0.0, \\'gratitude\\': 0.0, \\'approval\\': 0.0, \\'amusement\\': 0.0}The target text is :Hah hah I got the quote was mocking him - it was so perfect it thought it was genuine. Quality work :D.Choose the emotions from [neutral, admiration, gratitude, approval, amusement]. Output statisfies the following rules\\n Rule 1: Generate a dictionary of emotion probabilities in format of {\\'neutral\\': 0.1, \\'admiration\\':0.0, \\'gratitude\\':0.1, \\'approval\\':0.8, \\'amusement\\':0.0}. Rule 2: Ensure the sum of probability equal to 1.\\n Rule 3: Do not explain, only the dictionary.\\n Please check again whether your output follows the three rules.'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_format(text):\n",
    "    match = re.search(r\"\\{.*\\}\", text)\n",
    "    if match:\n",
    "        text = match.group(0)\n",
    "    result_dict = ast.literal_eval(text)\n",
    "    return result_dict\n",
    "def dictToList(dict, unique_labels):\n",
    "    prob_list = [dict[emo] for emo in unique_labels]\n",
    "    return prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch10(turn, example_text):\n",
    "    pred_emo = {}\n",
    "    log = {}\n",
    "    for i in range(10):\n",
    "        text = sample.loc[turn*10+i, 'text']\n",
    "        ID = sample.loc[turn*10+i, 'id']\n",
    "        try:\n",
    "            response, prompt = gemini_pred_emotion(text, example_text)\n",
    "            response = response.text.strip()\n",
    "            response = identify_format(response)\n",
    "            pred_emo[ID] = dictToList(response, unique_labels)\n",
    "            log[ID] = [prompt, text, response, sample.loc[turn*10+i, 'human_label']]\n",
    "        except:\n",
    "            try: \n",
    "                time.sleep(60)\n",
    "                response, prompt = gemini_pred_emotion(text, example_text)\n",
    "                response = response.text.strip()\n",
    "                \n",
    "                response = identify_format(response)\n",
    "                pred_emo[ID] = dictToList(response, unique_labels)\n",
    "\n",
    "                log[ID] = [prompt, text, response,sample.loc[turn*10+i, 'human_label']]\n",
    "            except:\n",
    "                print(ID,'Gemini api has an error.: ', text)\n",
    "                log[ID] = [text, prompt, sample.loc[turn*10+i, 'human_label']]\n",
    "                pred_emo[ID] = [1,0,0,0,0]\n",
    "    return pred_emo, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current turn: 0\n",
      "Current turn: 1\n",
      "Current turn: 2\n",
      "Current turn: 3\n",
      "edg883j Gemini api has an error.:  Why yes that ass is flawless\n",
      "Current turn: 4\n",
      "Current turn: 5\n",
      "Current turn: 6\n",
      "Current turn: 7\n",
      "Current turn: 8\n",
      "Current turn: 9\n",
      "Current turn: 10\n",
      "Current turn: 11\n",
      "Current turn: 12\n",
      "Current turn: 13\n",
      "Current turn: 14\n",
      "Current turn: 15\n",
      "Current turn: 16\n",
      "Current turn: 17\n",
      "Current turn: 18\n",
      "Current turn: 19\n",
      "Current turn: 20\n"
     ]
    }
   ],
   "source": [
    "pred_emo_t = {}\n",
    "log_t = {}\n",
    "for turn in range(0,21):\n",
    "    print(\"Current turn:\", turn)\n",
    "    time.sleep(turn)\n",
    "    pred_emo, log = batch10(turn, example_text)\n",
    "    pred_emo_t.update(pred_emo)\n",
    "    log_t.update(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"0908_fs15\"\n",
    "json.dump(pred_emo_t, open(f\"./goemotion_data/{folder_path}/pred.json\", 'w'), indent=4)\n",
    "json.dump(log_t, open(f\"./goemotion_data/{folder_path}/log.json\",'w'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_emo_t = json.load(open(\"./goemotion_data/0908_fs5/pred.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>human_label</th>\n",
       "      <th>gpt_label</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4971</td>\n",
       "      <td>Hah hah I got the quote was mocking him - it w...</td>\n",
       "      <td>edw68z8</td>\n",
       "      <td>0,1</td>\n",
       "      <td>admiration,amusement</td>\n",
       "      <td>admiration,amusement,surprise</td>\n",
       "      <td>0.065956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4636</td>\n",
       "      <td>I'll take a window seat for lmao at this.</td>\n",
       "      <td>eduh6da</td>\n",
       "      <td>1,27</td>\n",
       "      <td>amusement,neutral</td>\n",
       "      <td>amusement</td>\n",
       "      <td>0.065956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>Awesome! Welcome aboard! I'm glad your first g...</td>\n",
       "      <td>edybjan</td>\n",
       "      <td>0,15</td>\n",
       "      <td>admiration,gratitude</td>\n",
       "      <td>excitement,gratitude,joy</td>\n",
       "      <td>0.051469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2915</td>\n",
       "      <td>&gt;The only way change will happen is when the n...</td>\n",
       "      <td>efe4n7o</td>\n",
       "      <td>4,27</td>\n",
       "      <td>approval,neutral</td>\n",
       "      <td>curiosity,optimism</td>\n",
       "      <td>0.052823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2760</td>\n",
       "      <td>Not sure how kittens ‚Äúlol‚Äù but I‚Äôll take your ...</td>\n",
       "      <td>ed7lxtb</td>\n",
       "      <td>1,4</td>\n",
       "      <td>amusement,approval</td>\n",
       "      <td>amusement,confusion</td>\n",
       "      <td>0.063181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text       id  \\\n",
       "0        4971  Hah hah I got the quote was mocking him - it w...  edw68z8   \n",
       "1        4636          I'll take a window seat for lmao at this.  eduh6da   \n",
       "2        2018  Awesome! Welcome aboard! I'm glad your first g...  edybjan   \n",
       "3        2915  >The only way change will happen is when the n...  efe4n7o   \n",
       "4        2760  Not sure how kittens ‚Äúlol‚Äù but I‚Äôll take your ...  ed7lxtb   \n",
       "\n",
       "  label           human_label                      gpt_label   weights  \n",
       "0   0,1  admiration,amusement  admiration,amusement,surprise  0.065956  \n",
       "1  1,27     amusement,neutral                      amusement  0.065956  \n",
       "2  0,15  admiration,gratitude       excitement,gratitude,joy  0.051469  \n",
       "3  4,27      approval,neutral             curiosity,optimism  0.052823  \n",
       "4   1,4    amusement,approval            amusement,confusion  0.063181  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = []\n",
    "gt_labels = []\n",
    "for ID, dis in pred_fs.items():\n",
    "    pred_label.append(np.argmax(dis))\n",
    "    gt_labels.append(np.argmax(gt_sample_probs[ID]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5297579466524245"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_accuracy_score(gt_labels, pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fs = json.load(open(\"./goemotion_data/0908_fs15/pred.json\"))\n",
    "pred_zs = json.load(open(\"./goemotion_data/pred_5emo.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_5emo = json.load(open(\"./goemotion_data/log_5emo.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv(\"./data/goemotions_complete_test_annotations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2756    neutral\n",
       "Name: human_label, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.loc[all_df['text'] == log_5emo[\"0\"][1], \"human_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df_probs = {}\n",
    "for i, item in log_5emo.items():\n",
    "    emos = all_df.loc[all_df['text'] == item[1], 'human_label']\n",
    "    emotion_counts = Counter(emos.values[0].split(\",\"))\n",
    "    total_counts = sum(emotion_counts.values())\n",
    "    probs = {emo: round(emotion_counts[emo] / total_counts, 4) for emo in unique_labels}\n",
    "    gt_sample_probs[i] = [probs[emo] for emo in unique_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictToList(dict, unique_labels):\n",
    "    prob_list = [dict[emo] for emo in unique_labels]\n",
    "    return prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fs_list = {}\n",
    "for ID, dis in pred_zs.items():\n",
    "    pred_fs_list[ID] = dictToList(dis, unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44738323298668126"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_labels = []\n",
    "pred_labels = []\n",
    "for i in range(210):\n",
    "    gt_labels.append(np.argmax(gt_sample_probs[str(i)]))\n",
    "    pred_labels.append(np.argmax(pred_fs_list[str(i)]))\n",
    "balanced_accuracy_score(gt_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/Documents/Research Project/LLM_Ambiguous_Emotion/eval_metrics.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  KL_value = np.sum(np.where(a != 0, a * np.log(a / b), 0))\n",
      "/Users/macbookpro/Documents/Research Project/LLM_Ambiguous_Emotion/eval_metrics.py:12: RuntimeWarning: invalid value encountered in multiply\n",
      "  KL_value = np.sum(np.where(a != 0, a * np.log(a / b), 0))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.108278095238095"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_values = []\n",
    "for ID, probs in pred_emo_t.items():\n",
    "    kl_values.append(em.KL(gt_sample_probs[ID], pred_emo_t[ID]))\n",
    "np.mean(kl_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.595525238095238"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_values = []\n",
    "for ID, probs in pred_emo_t.items():\n",
    "    bc_di, bc_co = em.BC(gt_sample_probs[ID], pred_emo_t[ID])\n",
    "    bc_values.append(bc_co)\n",
    "np.mean(bc_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49140523809523806"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_square = []\n",
    "for ID, probs in pred_emo_t.items():\n",
    "    r_square.append(em.R(gt_sample_probs[ID], pred_emo_t[ID]))\n",
    "np.mean(r_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "truth = []\n",
    "for ID, item in pred_emo_t.items():\n",
    "    pred.append(item)\n",
    "for ID, item in gt_sample_probs.items():\n",
    "    truth.append(item)\n",
    "pred = np.array(pred)\n",
    "truth = np.array(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ECE(samples, trues, n_bims = 5):\n",
    "    # uniform binning approach with M number of bins\n",
    "    bin_boundaries = np.linspace(0, 1, n_bims + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    # get max probability per sample i\n",
    "    confidences = np.max(samples, axis=1)\n",
    "    # get predictions from confidences (positional in this case)\n",
    "    predicted_label = np.argmax(samples, axis=1)\n",
    "    true_labels = np.argmax(trues, axis=1)\n",
    "    \n",
    "    # get a boolean list of correct/false predictions\n",
    "    accuracies = predicted_label==true_labels\n",
    "\n",
    "    ece = np.zeros(1)\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        # determine if sample is in bin m (between bin lower & upper)\n",
    "        in_bin = np.logical_and(confidences > bin_lower.item(), confidences <= bin_upper.item())\n",
    "        # can calculate the empirical probability of a sample falling into bin m: (|Bm|/n)\n",
    "        prob_in_bin = in_bin.mean()\n",
    "\n",
    "        if prob_in_bin.item() > 0:\n",
    "            # get the accuracy of bin m: acc(Bm)\n",
    "            accuracy_in_bin = accuracies[in_bin].mean()\n",
    "            # get the average confidence of bin m: conf(Bm)\n",
    "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "            # calculate |acc(Bm) - conf(Bm)| * (|Bm|/n) for bin m and add to the total ECE\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prob_in_bin\n",
    "    return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36952381])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ECE(pred, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  50.48\n",
      "W-F1:  51.05\n"
     ]
    }
   ],
   "source": [
    "gt_label = []\n",
    "pred_label = []\n",
    "for i in range(210):\n",
    "    gt_label.append(np.argmax(pred[i]))\n",
    "    pred_label.append(np.argmax(truth[i]))\n",
    "\n",
    "print(\"acc: \", round(accuracy_score(gt_label, pred_label)*100, 2))\n",
    "print(\"W-F1: \", round(f1_score(gt_label, pred_label, average='weighted')*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
